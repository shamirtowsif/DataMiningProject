# -*- coding: utf-8 -*-
"""BS Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CC0X15dmEsV6Unks2FNakoXHpVao3KBY
"""

import pandas as pd
import numpy as np
from datetime import datetime
from tensorflow.keras.preprocessing.text import Tokenizer
import nltk
from nltk.corpus import stopwords
import spacy
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, GRU, SimpleRNN
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers
from tensorflow.keras.layers import concatenate

path = "/content/drive/My Drive/Deep Learning/fake.csv"
data = pd.read_csv(path)
train, valid, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])

print(len(train))
print(len(valid))
print(len(test))

train.head()

authors = {}

i = 1
for a in train["author"]:
  if a not in authors.keys():
    authors[a] = i
    i += 1

def get_author_id(x):
  if x in authors.keys():
    return authors[x]
  else:
    return 0

train["a"] = train["author"].apply(lambda x: get_author_id(x))
valid["a"] = valid["author"].apply(lambda x: get_author_id(x))
test["a"] = test["author"].apply(lambda x: get_author_id(x))

# def get_date(x):
#   x = datetime.strptime(x[:10], "%Y-%m-%d")
#   y = datetime.now()
#   z = y - x
#   return z.total_seconds()

# train["d"] = train["published"].apply(lambda x: get_date(x))
# valid["d"] = valid["published"].apply(lambda x: get_date(x))
# test["d"] = test["published"].apply(lambda x: get_date(x))

# train["c"] = train["crawled"].apply(lambda x: get_date(x))
# valid["c"] = valid["crawled"].apply(lambda x: get_date(x))
# test["c"] = test["crawled"].apply(lambda x: get_date(x))

train["t1"] = train["title"].apply(lambda x: str(x))
valid["t1"] = valid["title"].apply(lambda x: str(x))
test["t1"] = test["title"].apply(lambda x: str(x))

tokenizer1 = Tokenizer()
tokenizer1.fit_on_texts(train["t1"])
vocab1 = tokenizer1.word_index

nltk.download("stopwords")

def get_word_ids1(x):
  words = [w for w in x.split(' ') if w not in stopwords.words("english")]
  n_words = []
  for w in words:
    if w in vocab1.keys():
      n_words.append(vocab1[w])
    else:
      n_words.append(0)
  return n_words

train["t"] = train["t1"].apply(lambda x: get_word_ids1(x))
valid["t"] = valid["t1"].apply(lambda x: get_word_ids1(x))
test["t"] = test["t1"].apply(lambda x: get_word_ids1(x))

# train["x1"] = train["text"].apply(lambda x: str(x))
# valid["x1"] = valid["text"].apply(lambda x: str(x))
# test["x1"] = test["text"].apply(lambda x: str(x))

# tokenizer2 = Tokenizer()
# tokenizer2.fit_on_texts(train["x1"])
# vocab2 = tokenizer2.word_index

# def get_word_ids2(x):
#   words = [w for w in x.split(' ') if w not in stopwords.words("english")]
#   n_words = []
#   for w in words:
#     if w in vocab2.keys():
#       n_words.append(vocab2[w])
#     else:
#       n_words.append(0)
#   return n_words

# train["x"] = train["x1"].apply(lambda x: get_word_ids2(x))
# valid["x"] = valid["x1"].apply(lambda x: get_word_ids2(x))
# test["x"] = test["x1"].apply(lambda x: get_word_ids2(x))

# train["tt1"] = train["thread_title"].apply(lambda x: str(x))
# valid["tt1"] = valid["thread_title"].apply(lambda x: str(x))
# test["tt1"] = test["thread_title"].apply(lambda x: str(x))

# tokenizer3 = Tokenizer()
# tokenizer3.fit_on_texts(train["tt1"])
# vocab3 = tokenizer3.word_index

# def get_word_ids3(x):
#   words = [w for w in x.split(' ') if w not in stopwords.words("english")]
#   n_words = []
#   for w in words:
#     if w in vocab3.keys():
#       n_words.append(vocab3[w])
#     else:
#       n_words.append(0)
#   return n_words

# train["tt"] = train["tt1"].apply(lambda x: get_word_ids3(x))
# valid["tt"] = valid["tt1"].apply(lambda x: get_word_ids3(x))
# test["tt"] = test["tt1"].apply(lambda x: get_word_ids3(x))

languages = {}

i = 1
for a in train["language"]:
  if a not in languages.keys():
    languages[a] = i
    i += 1

def get_language_id(x):
  if x in languages.keys():
    return languages[x]
  else:
    return 0

train["l"] = train["language"].apply(lambda x: get_language_id(x))
valid["l"] = valid["language"].apply(lambda x: get_language_id(x))
test["l"] = test["language"].apply(lambda x: get_language_id(x))

sites = {}

i = 1
for a in train["site_url"]:
  if a not in sites.keys():
    sites[a] = i
    i += 1

def get_site_id(x):
  if x in sites.keys():
    return sites[x]
  else:
    return 0

train["s"] = train["author"].apply(lambda x: get_site_id(x))
valid["s"] = valid["author"].apply(lambda x: get_site_id(x))
test["s"] = test["author"].apply(lambda x: get_site_id(x))

countries = {}

i = 1
for a in train["country"]:
  if a not in countries.keys():
    countries[a] = i
    i += 1

def get_country_id(x):
  if x in countries.keys():
    return countries[x]
  else:
    return 0

train["n"] = train["country"].apply(lambda x: get_country_id(x))
valid["n"] = valid["country"].apply(lambda x: get_country_id(x))
test["n"] = test["country"].apply(lambda x: get_country_id(x))

domains = {}

i = 1
for a in train["domain_rank"]:
  if a not in domains.keys():
    domains[a] = i
    i += 1

def get_domain_id(x):
  if x in domains.keys():
    return domains[x]
  else:
    return 0

train["m"] = train["domain_rank"].apply(lambda x: get_domain_id(x))
valid["m"] = valid["domain_rank"].apply(lambda x: get_domain_id(x))
test["m"] = test["domain_rank"].apply(lambda x: get_domain_id(x))

# def get_image_id(a):
#   if isinstance(a, str): return 1
#   else: return 0 

# train["i"] = train["main_img_url"].apply(lambda x: get_image_id(x))
# valid["i"] = valid["main_img_url"].apply(lambda x: get_image_id(x))
# test["i"] = test["main_img_url"].apply(lambda x: get_image_id(x))

types = {'bs': 0, 'bias': 1, 'conspiracy': 2, 'hate': 3, 'satire': 4, 'state': 5, 'junksci': 6, 'fake': 7}

train["p"] = train["type"].apply(lambda x: types[x])
valid["p"] = valid["type"].apply(lambda x: types[x])
test["p"] = test["type"].apply(lambda x: types[x])

mxlen1, mxlen2, mxlen3 = 0, 0, 0
for l in train["t"]:
  if len(l) > mxlen1: mxlen1 = len(l)
# for l in train["x"]:
#   if len(l) > mxlen2: mxlen2 = len(l)
# for l in train["tt"]:
#   if len(l) > mxlen3: mxlen3 = len(l)

t_train = sequence.pad_sequences(train["t"], maxlen=mxlen1, padding="post", truncating="post")
t_valid = sequence.pad_sequences(valid["t"], maxlen=mxlen1, padding="post", truncating="post")
t_test = sequence.pad_sequences(test["t"], maxlen=mxlen1, padding="post", truncating="post")

# x_train = sequence.pad_sequences(train["x"], maxlen=mxlen2, padding="post", truncating="post")
# x_valid = sequence.pad_sequences(valid["x"], maxlen=mxlen2, padding="post", truncating="post")
# x_test = sequence.pad_sequences(test["x"], maxlen=mxlen2, padding="post", truncating="post")

# tt_train = sequence.pad_sequences(train["tt"], maxlen=mxlen3, padding="post", truncating="post")
# tt_valid = sequence.pad_sequences(valid["tt"], maxlen=mxlen3, padding="post", truncating="post")
# tt_test = sequence.pad_sequences(test["tt"], maxlen=mxlen3, padding="post", truncating="post")

a_train = to_categorical(train["a"], num_classes=len(authors)+1)
l_train = to_categorical(train["l"], num_classes=len(languages)+1)
s_train = to_categorical(train["s"], num_classes=len(sites)+1)
n_train = to_categorical(train["n"], num_classes=len(countries)+1)
m_train = to_categorical(train["m"], num_classes=len(domains)+1)

train_meta = np.hstack((a_train, l_train, s_train, n_train, m_train))

# train_meta.shape

a_valid = to_categorical(valid["a"], num_classes=len(authors)+1)
l_valid = to_categorical(valid["l"], num_classes=len(languages)+1)
s_valid = to_categorical(valid["s"], num_classes=len(sites)+1)
n_valid = to_categorical(valid["n"], num_classes=len(countries)+1)
m_valid = to_categorical(valid["m"], num_classes=len(domains)+1)

valid_meta = np.hstack((a_valid, l_valid, s_valid, n_valid, m_valid))

a_test = to_categorical(test["a"], num_classes=len(authors)+1)
l_test = to_categorical(test["l"], num_classes=len(languages)+1)
s_test = to_categorical(test["s"], num_classes=len(sites)+1)
n_test = to_categorical(test["n"], num_classes=len(countries)+1)
m_test = to_categorical(test["m"], num_classes=len(domains)+1)

test_meta = np.hstack((a_test, l_test, s_test, n_test, m_test))

y_train = to_categorical(train["p"], num_classes=8)
y_valid = to_categorical(valid["p"], num_classes=8)
y_test = test["p"].to_numpy()

t_embeddings = np.zeros((len(vocab1), 100))
with open("/content/drive/My Drive/Deep Learning/glove.6B.100d.txt", "r") as f:
  for l in f:
    l = l.split(" ")
    if l[0] in vocab1.keys():
      t_embeddings[vocab1[l[0]]] = l[1:]

# x_embeddings = np.zeros((len(vocab2)+1, 100))
# with open("/content/drive/My Drive/Deep Learning/glove.6B.100d.txt", "r") as f:
#   for l in f:
#     l = l.split(" ")
#     if l[0] in vocab2.keys():
#       x_embeddings[vocab2[l[0]]] = l[1:]

# tt_embeddings = np.zeros((len(vocab3)+1, 100))
# with open("/content/drive/My Drive/Deep Learning/glove.6B.100d.txt", "r") as f:
#   for l in f:
#     l = l.split(" ")
#     if l[0] in vocab3.keys():
#       tt_embeddings[vocab3[l[0]]] = l[1:]

x1 = Input(shape=(mxlen1,), dtype="int32", name="input1")
x2 = Embedding(len(vocab1), 100, input_length=mxlen1, weights=[t_embeddings], trainable=False)(x1)
x3 = LSTM(100)(x2)

x4 = Input(shape=(4600,), dtype="int32", name="meta_input")
x5 = Dense(64, activation="relu")(x4)

# x4 = Input(shape=(mxlen2,), dtype="int32", name="input2")
# x5 = Embedding(len(vocab2), len(vocab2), input_length=mxlen2, weights=[x_embeddings], trainable=False)(x4)
# x6 = LSTM(100)(x5)

# x7 = Input(shape=(mxlen3,), dtype="int32", name="input3")
# x8 = Embedding(len(vocab3)+1, len(vocab3), input_length=mxlen3, weights=[tt_embeddings], trainable=False)(x7)
# x9 = LSTM(100)(x8)

# x10 = Input(shape=(train_meta.shape[1],), dtype="int32", name="input4")
# x11 = Dense(64, activation="relu")(x10)

x6 = concatenate([x3, x5])

y = Dense(8, activation="softmax", name="output")(x6)

model = Model(inputs=[x1, x4], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": t_train, "meta_input": train_meta}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": t_valid, "meta_input": valid_meta}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([t_test, test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

x1 = Input(shape=(mxlen1,), dtype="int32", name="input1")
x2 = Embedding(len(vocab1), 100, input_length=mxlen1, weights=[t_embeddings], trainable=False)(x1)
x3 = Bidirectional(LSTM(100))(x2)

x4 = Input(shape=(4600,), dtype="int32", name="meta_input")
x5 = Dense(64, activation="relu")(x4)

# x4 = Input(shape=(mxlen2,), dtype="int32", name="input2")
# x5 = Embedding(len(vocab2), len(vocab2), input_length=mxlen2, weights=[x_embeddings], trainable=False)(x4)
# x6 = LSTM(100)(x5)

# x7 = Input(shape=(mxlen3,), dtype="int32", name="input3")
# x8 = Embedding(len(vocab3)+1, len(vocab3), input_length=mxlen3, weights=[tt_embeddings], trainable=False)(x7)
# x9 = LSTM(100)(x8)

# x10 = Input(shape=(train_meta.shape[1],), dtype="int32", name="input4")
# x11 = Dense(64, activation="relu")(x10)

x6 = concatenate([x3, x5])

y = Dense(8, activation="softmax", name="output")(x6)

model = Model(inputs=[x1, x4], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": t_train, "meta_input": train_meta}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": t_valid, "meta_input": valid_meta}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([t_test, test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

x1 = Input(shape=(mxlen1,), dtype="int32", name="input1")
x2 = Embedding(len(vocab1), 100, input_length=mxlen1, weights=[t_embeddings], trainable=False)(x1)
x3 = GRU(100)(x2)

x4 = Input(shape=(4600,), dtype="int32", name="meta_input")
x5 = Dense(64, activation="relu")(x4)

# x4 = Input(shape=(mxlen2,), dtype="int32", name="input2")
# x5 = Embedding(len(vocab2), len(vocab2), input_length=mxlen2, weights=[x_embeddings], trainable=False)(x4)
# x6 = LSTM(100)(x5)

# x7 = Input(shape=(mxlen3,), dtype="int32", name="input3")
# x8 = Embedding(len(vocab3)+1, len(vocab3), input_length=mxlen3, weights=[tt_embeddings], trainable=False)(x7)
# x9 = LSTM(100)(x8)

# x10 = Input(shape=(train_meta.shape[1],), dtype="int32", name="input4")
# x11 = Dense(64, activation="relu")(x10)

x6 = concatenate([x3, x5])

y = Dense(8, activation="softmax", name="output")(x6)

model = Model(inputs=[x1, x4], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": t_train, "meta_input": train_meta}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": t_valid, "meta_input": valid_meta}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([t_test, test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

x1 = Input(shape=(mxlen1,), dtype="int32", name="input1")
x2 = Embedding(len(vocab1), 100, input_length=mxlen1, weights=[t_embeddings], trainable=False)(x1)
x3 = Bidirectional(GRU(100))(x2)

x4 = Input(shape=(4600,), dtype="int32", name="meta_input")
x5 = Dense(64, activation="relu")(x4)

# x4 = Input(shape=(mxlen2,), dtype="int32", name="input2")
# x5 = Embedding(len(vocab2), len(vocab2), input_length=mxlen2, weights=[x_embeddings], trainable=False)(x4)
# x6 = LSTM(100)(x5)

# x7 = Input(shape=(mxlen3,), dtype="int32", name="input3")
# x8 = Embedding(len(vocab3)+1, len(vocab3), input_length=mxlen3, weights=[tt_embeddings], trainable=False)(x7)
# x9 = LSTM(100)(x8)

# x10 = Input(shape=(train_meta.shape[1],), dtype="int32", name="input4")
# x11 = Dense(64, activation="relu")(x10)

x6 = concatenate([x3, x5])

y = Dense(8, activation="softmax", name="output")(x6)

model = Model(inputs=[x1, x4], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": t_train, "meta_input": train_meta}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": t_valid, "meta_input": valid_meta}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([t_test, test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

x1 = Input(shape=(mxlen1,), dtype="int32", name="input1")
x2 = Embedding(len(vocab1), 100, input_length=mxlen1, weights=[t_embeddings], trainable=False)(x1)
x3 = SimpleRNN(100)(x2)

x4 = Input(shape=(4600,), dtype="int32", name="meta_input")
x5 = Dense(64, activation="relu")(x4)

# x4 = Input(shape=(mxlen2,), dtype="int32", name="input2")
# x5 = Embedding(len(vocab2), len(vocab2), input_length=mxlen2, weights=[x_embeddings], trainable=False)(x4)
# x6 = LSTM(100)(x5)

# x7 = Input(shape=(mxlen3,), dtype="int32", name="input3")
# x8 = Embedding(len(vocab3)+1, len(vocab3), input_length=mxlen3, weights=[tt_embeddings], trainable=False)(x7)
# x9 = LSTM(100)(x8)

# x10 = Input(shape=(train_meta.shape[1],), dtype="int32", name="input4")
# x11 = Dense(64, activation="relu")(x10)

x6 = concatenate([x3, x5])

y = Dense(8, activation="softmax", name="output")(x6)

model = Model(inputs=[x1, x4], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": t_train, "meta_input": train_meta}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": t_valid, "meta_input": valid_meta}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([t_test, test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

x1 = Input(shape=(mxlen1,), dtype="int32", name="input1")
x2 = Embedding(len(vocab1), 100, input_length=mxlen1, weights=[t_embeddings], trainable=False)(x1)
x3 = Bidirectional(SimpleRNN(100))(x2)

x4 = Input(shape=(4600,), dtype="int32", name="meta_input")
x5 = Dense(64, activation="relu")(x4)

# x4 = Input(shape=(mxlen2,), dtype="int32", name="input2")
# x5 = Embedding(len(vocab2), len(vocab2), input_length=mxlen2, weights=[x_embeddings], trainable=False)(x4)
# x6 = LSTM(100)(x5)

# x7 = Input(shape=(mxlen3,), dtype="int32", name="input3")
# x8 = Embedding(len(vocab3)+1, len(vocab3), input_length=mxlen3, weights=[tt_embeddings], trainable=False)(x7)
# x9 = LSTM(100)(x8)

# x10 = Input(shape=(train_meta.shape[1],), dtype="int32", name="input4")
# x11 = Dense(64, activation="relu")(x10)

x6 = concatenate([x3, x5])

y = Dense(8, activation="softmax", name="output")(x6)

model = Model(inputs=[x1, x4], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": t_train, "meta_input": train_meta}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": t_valid, "meta_input": valid_meta}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([t_test, test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")
