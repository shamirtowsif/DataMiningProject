# -*- coding: utf-8 -*-
"""Buzzfeed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dhP8vGgV-7msosER3kwjSA1EvTysk6zC
"""

import pandas as pd
import numpy as np
import math
from datetime import datetime
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras import optimizers

path = "/content/drive/My Drive/Deep Learning/facebook-fact-check.csv"
data = pd.read_csv(path)
train, valid, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])

print(len(train))
print(len(valid))
print(len(test))

train.head()

accounts = {}

i = 1
for account in train["account_id"]:
  if account not in accounts.keys():
    accounts[account] = i
    i += 1

def get_account_id(x):
  if x in accounts.keys():
    return accounts[x]
  else:
    return 0

train["a"] = train["account_id"].apply(lambda x: get_account_id(x))
valid["a"] = valid["account_id"].apply(lambda x: get_account_id(x))
test["a"] = test["account_id"].apply(lambda x: get_account_id(x))

cats = {}

i = 1
for cat in train["Category"]:
  if cat not in cats.keys():
    cats[cat] = i
    i += 1

def get_cat_id(x):
  if x in cats.keys():
    return cats[x]
  else:
    return 0

train["c"] = train["Category"].apply(lambda x: get_cat_id(x))
valid["c"] = valid["Category"].apply(lambda x: get_cat_id(x))
test["c"] = test["Category"].apply(lambda x: get_cat_id(x))

pages = {}

i = 1
for page in train["Page"]:
  if page not in pages.keys():
    pages[page] = i
    i += 1

def get_page_id(x):
  if x in pages.keys():
    return pages[x]
  else:
    return 0

train["p"] = train["Page"].apply(lambda x: get_page_id(x))
valid["p"] = valid["Page"].apply(lambda x: get_page_id(x))
test["p"] = test["Page"].apply(lambda x: get_page_id(x))

def get_date_id(x):
  x = datetime.strptime(x, "%Y-%m-%d")
  y = datetime.now()
  z = y - x
  return z.total_seconds()

train["d"] = train["Date Published"].apply(lambda x: get_date_id(x))
valid["d"] = valid["Date Published"].apply(lambda x: get_date_id(x))
test["d"] = test["Date Published"].apply(lambda x: get_date_id(x))

types = {}

i = 1
for tp in train["Post Type"]:
  if tp not in types.keys():
    types[tp] = i
    i += 1

def get_type_id(x):
  if x in types.keys():
    return types[x]
  else:
    return 0

train["t"] = train["Post Type"].apply(lambda x: get_type_id(x))
valid["t"] = valid["Post Type"].apply(lambda x: get_type_id(x))
test["t"] = test["Post Type"].apply(lambda x: get_type_id(x))

ratings = {'mostly true': 0, 'mostly false': 1, 'mixture of true and false': 2, 'no factual content': 3}

train["r"] = train["Rating"].apply(lambda x: ratings[x])
valid["r"] = valid["Rating"].apply(lambda x: ratings[x])
test["r"] = test["Rating"].apply(lambda x: ratings[x])

def get_db_id(x):
  if x == "yes": return 1
  else: return 0

train["db"] = train["Debate"].apply(lambda x: get_db_id(x))
valid["db"] = valid["Debate"].apply(lambda x: get_db_id(x))
test["db"] = test["Debate"].apply(lambda x: get_db_id(x))

def get_shr_id(x):
  if math.isnan(x):
    return 0
  else:
    return x

train["s"] = train["share_count"].apply(lambda x: get_shr_id(x))
valid["s"] = valid["share_count"].apply(lambda x: get_shr_id(x))
test["s"] = test["share_count"].apply(lambda x: get_shr_id(x))

def get_reaction_id(x):
  if math.isnan(x):
    return 0
  else:
    return x

train["rc"] = train["reaction_count"].apply(lambda x: get_reaction_id(x))
valid["rc"] = valid["reaction_count"].apply(lambda x: get_reaction_id(x))
test["rc"] = test["reaction_count"].apply(lambda x: get_reaction_id(x))

def get_comment_id(x):
  if math.isnan(x):
    return 0
  else:
    return x

train["m"] = train["comment_count"].apply(lambda x: get_comment_id(x))
valid["m"] = valid["comment_count"].apply(lambda x: get_comment_id(x))
test["m"] = test["comment_count"].apply(lambda x: get_comment_id(x))

a_train = to_categorical(train["a"], num_classes=len(accounts)+1)
c_train = to_categorical(train["c"], num_classes=len(cats)+1)
p_train = to_categorical(train["p"], num_classes=len(pages)+1)
d_train = train["d"]
t_train = to_categorical(train["t"], num_classes=len(types)+1)
db_train = to_categorical(train["db"], num_classes=2)
s_train = train["s"]
rc_train = train["rc"]
m_train = train["m"]

x_train = np.hstack((a_train, c_train, p_train, t_train, db_train))

a_valid = to_categorical(valid["a"], num_classes=len(accounts)+1)
c_valid = to_categorical(valid["c"], num_classes=len(cats)+1)
p_valid = to_categorical(valid["p"], num_classes=len(pages)+1)
d_valid = valid["d"]
t_valid = to_categorical(valid["t"], num_classes=len(types)+1)
db_valid = to_categorical(valid["db"], num_classes=2)
s_valid = valid["s"]
rc_valid = valid["rc"]
m_valid = valid["m"]

x_valid = np.hstack((a_valid, c_valid, p_valid, t_valid, db_valid))

a_test = to_categorical(test["a"], num_classes=len(accounts)+1)
c_test = to_categorical(test["c"], num_classes=len(cats)+1)
p_test = to_categorical(test["p"], num_classes=len(pages)+1)
d_test = test["d"]
t_test = to_categorical(test["t"], num_classes=len(types)+1)
db_test = to_categorical(test["db"], num_classes=2)
s_test = test["s"]
rc_test = test["rc"]
m_test = test["m"]

x_test = np.hstack((a_test, c_test, p_test, t_test, db_test))

y_train = to_categorical(train["r"], num_classes=4)
y_valid = to_categorical(valid["r"], num_classes=4)
y_test = test["r"].to_numpy()

y_train.shape
d_train.shape

x1 = Input(shape=(x_train.shape[1],), dtype="int32", name="input1")
x2 = Dense(64, activation='relu')(x1)

x3 = Input(shape=(1,), dtype="int32", name="input2")
x4 = Dense(64, activation='relu')(x3)

x5 = Input(shape=(1,), dtype="int32", name="input3")
x6 = Dense(64, activation='relu')(x5)

x7 = Input(shape=(1,), dtype="int32", name="input4")
x8 = Dense(64, activation='relu')(x7)

x9 = Input(shape=(1,), dtype="int32", name="input5")
x10 = Dense(64, activation='relu')(x9)

x11 = concatenate([x2, x4, x6, x8, x10])
y = Dense(4, activation="softmax", name="output")(x11)


model = Model(inputs=[x1, x3, x5, x7, x9], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"])
model.fit({"input1": x_train, "input2": d_train, "input3": s_train, "input4": rc_train, "input5": m_train}, {"output": y_train}, epochs=30, batch_size=40, verbose=1, validation_data=({"input1": x_valid, "input2": d_valid, "input3": s_valid, "input4": rc_valid, "input5": m_valid}, {"output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, d_test, s_test, rc_test, m_test])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")