# -*- coding: utf-8 -*-
"""Liar Liar - DS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12RvmflYmvkTG_twJPHVclhCUtmV7G0zb
"""

import pandas as pd
import numpy as np
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, GRU, SimpleRNN
from tensorflow.keras.models import Model
from tensorflow.keras.layers import concatenate
import nltk
from nltk.corpus import stopwords
import spacy
from tensorflow.keras.utils import to_categorical

#import data from the files
#how pands read_table works: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html

train = "/content/drive/My Drive/Deep Learning/train.tsv"
val = "/content/drive/My Drive/Deep Learning/valid.tsv"
test = "/content/drive/My Drive/Deep Learning/test.tsv"

train_data = pd.read_table(train, names=["id", "label", "statement", "subject", "speaker", "job", "state", "party", "barely-true", "false", "half-true", "mostly-true", "pants-on-fire", "venue"])
valid_data = pd.read_table(val, names=["id", "label", "statement", "subject", "speaker", "job", "state", "party", "barely-true", "false", "half-true", "mostly-true", "pants-on-fire", "venue"])
test_data = pd.read_table(test, names=["id", "label", "statement", "subject", "speaker", "job", "state", "party", "barely-true", "false", "half-true", "mostly-true", "pants-on-fire", "venue"])

# use a numrical value for the label instead of string for the target value

labels = {"pants-fire": 0,  "false": 1,  "barely-true": 2, "half-true": 3, "mostly-true": 4, "true": 5}

train_data["output"] = train_data["label"].apply(lambda x: labels[x])
valid_data["output"] = valid_data["label"].apply(lambda x: labels[x])
test_data["output"] = test_data["label"].apply(lambda x: labels[x])

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data["statement"])
vocab = tokenizer.word_index

nltk.download("stopwords")

def get_word_ids(x):
  words = [w for w in x.split(' ') if w not in stopwords.words("english")]
  n_words = []
  for w in words:
    if w in vocab.keys():
      n_words.append(vocab[w])
    else:
      n_words.append(0)
  return n_words

train_data["word_id"] = train_data["statement"].apply(lambda x: get_word_ids(x))
valid_data["word_id"] = valid_data["statement"].apply(lambda x: get_word_ids(x))
test_data["word_id"] = test_data["statement"].apply(lambda x: get_word_ids(x))

subjects = {}

i = 1
for subject in train_data["subject"]:
  if isinstance(subject, str):
    c_subjects = subject.split(',')
    for c_subject in c_subjects:
      if c_subject not in subjects.keys():
        subjects[c_subject] = i
        i += 1

def get_subject_ids(x):
  if isinstance(x, str):
    x = x.split(',')
    ids = []
    for c_subject in x:
      if c_subject in subjects.keys():
        ids.append(subjects[c_subject])
      else:
        ids.append(0)
    return ids
  else:
    return [0]

train_data["subject_id"] = train_data["subject"].apply(lambda x: get_subject_ids(x))
valid_data["subject_id"] = valid_data["subject"].apply(lambda x: get_subject_ids(x))
test_data["subject_id"] = test_data["subject"].apply(lambda x: get_subject_ids(x))

speakers = {}

i = 1
for speaker in train_data["speaker"]:
  if speaker not in speakers.keys():
    speakers[speaker] = i
    i += 1

def get_speaker_id(x):
  if x in speakers.keys():
    return speakers[x]
  else:
    return 0

train_data["speaker_id"] = train_data["speaker"].apply(lambda x: get_speaker_id(x))
valid_data["speaker_id"] = valid_data["speaker"].apply(lambda x: get_speaker_id(x))
test_data["speaker_id"] = test_data["speaker"].apply(lambda x: get_speaker_id(x))

jobs = {}

i = 1
for job in train_data["job"]:
  if job not in jobs.keys():
    jobs[job] = i
    i += 1

def get_job_id(x):
  if x in jobs.keys():
    return jobs[x]
  else:
    return 0

train_data["job_id"] = train_data["job"].apply(lambda x: get_job_id(x))
valid_data["job_id"] = valid_data["job"].apply(lambda x: get_job_id(x))
test_data["job_id"] = test_data["job"].apply(lambda x: get_job_id(x))

states = {}

i = 1
for state in train_data["state"]:
  if state not in states.keys():
    states[state] = i
    i += 1

def get_state_id(x):
  if x in states.keys():
    return states[x]
  else:
    return 0

train_data["state_id"] = train_data["state"].apply(lambda x: get_state_id(x))
valid_data["state_id"] = valid_data["state"].apply(lambda x: get_state_id(x))
test_data["state_id"] = test_data["state"].apply(lambda x: get_state_id(x))

partys = {}

i = 1
for party in train_data["party"]:
  if party not in partys.keys():
    partys[party] = i
    i += 1

def get_party_id(x):
  if x in partys.keys():
    return partys[x]
  else:
    return 0

train_data["party_id"] = train_data["party"].apply(lambda x: get_party_id(x))
valid_data["party_id"] = valid_data["party"].apply(lambda x: get_party_id(x))
test_data["party_id"] = test_data["party"].apply(lambda x: get_party_id(x))

venues = {}

i = 1
for venue in train_data["venue"]:
  if venue not in venues.keys():
    venues[venue] = i
    i += 1

def get_venue_id(x):
  if x in venues.keys():
    return venues[x]
  else:
    return 0

train_data["venue_id"] = train_data["venue"].apply(lambda x: get_venue_id(x))
valid_data["venue_id"] = valid_data["venue"].apply(lambda x: get_venue_id(x))
test_data["venue_id"] = test_data["venue"].apply(lambda x: get_venue_id(x))

nlp = spacy.load("en")

pos = {'VERB': 1, 'DET': 2, 'PROPN': 3, 'ADJ': 4, 'NOUN': 5, 'PUNCT': 6, 'ADP': 7, 'ADV': 8, 'AUX': 9, 'PRON': 10, 'PART': 11, 'NUM': 12, 'SCONJ': 13, 'SYM': 14, 'CCONJ': 15, 'INTJ': 16, 'X': 17, 'SPACE': 18}

# i = 1
# for l in train_data["statement"]:
#   for token in nlp(l):
#     if token.pos_ not in pos.keys():
#       pos[token.pos_] = i
#       i += 1

dep = {'ROOT': 1, 'det': 2, 'compound': 3, 'nmod': 4, 'amod': 5, 'nsubj': 6, 'ccomp': 7, 'punct': 8, 'dobj': 9, 'prep': 10, 'pobj': 11, 'advmod': 12, 'advcl': 13, 'prt': 14, 'aux': 15, 'xcomp': 16, 'pcomp': 17, 'dative': 18, 'acomp': 19, 'poss': 20, 'nummod': 21, 'acl': 22, 'neg': 23, 'relcl': 24, 'attr': 25, 'npadvmod': 26, 'appos': 27, 'quantmod': 28, 'cc': 29, 'conj': 30, 'nsubjpass': 31, 'auxpass': 32, 'mark': 33, 'intj': 34, 'dep': 35, 'case': 36, 'agent': 37, 'predet': 38, 'expl': 39, 'oprd': 40, 'csubj': 41, 'parataxis': 42, 'preconj': 43, '': 44, 'csubjpass': 45, 'meta': 46}

# i = 1
# for l in train_data["statement"]:
#   for token in nlp(l):
#     if token.dep_ not in dep.keys():
#       dep[token.dep_] = i
#       i += 1

# print(dep)

def get_pos_ids(x):
  ids = []
  for token in nlp(x):
    if token.pos_ in pos.keys():
      ids.append(pos[token.pos_])
    else:
      ids.append(0)
  return ids


train_data["pos_id"] = train_data["statement"].apply(lambda x: get_pos_ids(x))
valid_data["pos_id"] = valid_data["statement"].apply(lambda x: get_pos_ids(x))
test_data["pos_id"] = test_data["statement"].apply(lambda x: get_pos_ids(x))

def get_dep_ids(x):
  ids = []
  for token in nlp(x):
    if token.dep_ in dep.keys():
      ids.append(dep[token.dep_])
    else:
      ids.append(0)
  return ids


train_data["dep_id"] = train_data["statement"].apply(lambda x: get_dep_ids(x))
valid_data["dep_id"] = valid_data["statement"].apply(lambda x: get_dep_ids(x))
test_data["dep_id"] = test_data["statement"].apply(lambda x: get_dep_ids(x))

mxlen = 0
for l in train_data["word_id"]:
  if len(l) > mxlen: mxlen = len(l)

x_train = sequence.pad_sequences(train_data["word_id"], maxlen=mxlen, padding="post", truncating="post")
x_valid = sequence.pad_sequences(valid_data["word_id"], maxlen=mxlen, padding="post", truncating="post")
x_test = sequence.pad_sequences(test_data["word_id"], maxlen=mxlen, padding="post", truncating="post")

x_train_pos = sequence.pad_sequences(train_data["pos_id"], maxlen=mxlen, padding="post", truncating="post")
x_valid_pos = sequence.pad_sequences(valid_data["pos_id"], maxlen=mxlen, padding="post", truncating="post")
x_test_pos = sequence.pad_sequences(test_data["pos_id"], maxlen=mxlen, padding="post", truncating="post")

x_train_dep = sequence.pad_sequences(train_data["dep_id"], maxlen=mxlen, padding="post", truncating="post")
x_valid_dep = sequence.pad_sequences(valid_data["dep_id"], maxlen=mxlen, padding="post", truncating="post")
x_test_dep = sequence.pad_sequences(test_data["dep_id"], maxlen=mxlen, padding="post", truncating="post")

subject_train = train_data["subject_id"].apply(lambda x: to_categorical(x, num_classes=len(subjects)+1).sum(axis=0)).apply(pd.Series).to_numpy()
speaker_train = to_categorical(train_data["speaker_id"], num_classes=len(speakers)+1)
job_train = to_categorical(train_data["job_id"], num_classes=len(jobs)+1)
state_train = to_categorical(train_data["state_id"], num_classes=len(states)+1)
party_train = to_categorical(train_data["party_id"], num_classes=len(partys)+1)
venue_train = to_categorical(train_data["venue_id"], num_classes=len(venues)+1)

x_train_meta = np.hstack((subject_train, speaker_train, job_train, state_train, party_train, venue_train))

subject_valid = valid_data["subject_id"].apply(lambda x: to_categorical(x, num_classes=len(subjects)+1).sum(axis=0)).apply(pd.Series).to_numpy()
speaker_valid = to_categorical(valid_data["speaker_id"], num_classes=len(speakers)+1)
job_valid = to_categorical(valid_data["job_id"], num_classes=len(jobs)+1)
state_valid = to_categorical(valid_data["state_id"], num_classes=len(states)+1)
party_valid = to_categorical(valid_data["party_id"], num_classes=len(partys)+1)
venue_valid = to_categorical(valid_data["venue_id"], num_classes=len(venues)+1)

x_valid_meta = np.hstack((subject_valid, speaker_valid, job_valid, state_valid, party_valid, venue_valid))

subject_test = test_data["subject_id"].apply(lambda x: to_categorical(x, num_classes=len(subjects)+1).sum(axis=0)).apply(pd.Series).to_numpy()
speaker_test = to_categorical(test_data["speaker_id"], num_classes=len(speakers)+1)
job_test = to_categorical(test_data["job_id"], num_classes=len(jobs)+1)
state_test = to_categorical(test_data["state_id"], num_classes=len(states)+1)
party_test = to_categorical(test_data["party_id"], num_classes=len(partys)+1)
venue_test = to_categorical(test_data["venue_id"], num_classes=len(venues)+1)

x_test_meta = np.hstack((subject_test, speaker_test, job_test, state_test, party_test, venue_test))

y_train = to_categorical(train_data["output"], num_classes=6)
y_valid = to_categorical(valid_data["output"], num_classes=6)
y_test = test_data["output"].to_numpy()

print(x_train.shape)
print(x_valid.shape)
print(x_test.shape)

print(x_train_pos.shape)
print(x_valid_pos.shape)
print(x_test_pos.shape)

print(x_train_dep.shape)
print(x_valid_dep.shape)
print(x_test_dep.shape)

print(x_train_meta.shape)
print(x_valid_meta.shape)
print(x_test_meta.shape)

embeddings = np.zeros((len(vocab), 100))
with open("/content/drive/My Drive/Deep Learning/glove.6B.100d.txt", "r") as f:
  for l in f:
    l = l.split(" ")
    if l[0] in vocab.keys():
      embeddings[vocab[l[0]]] = l[1:]

pos_embeddings = np.identity(len(pos)+1, dtype="int32")
dep_embeddings = np.identity(len(dep)+1, dtype="int32")

word_input = Input(shape=(mxlen,), dtype="int32", name="word_input")
x1 = Embedding(len(vocab), 100, input_length=mxlen, weights=[embeddings], trainable=False)(word_input)
x1 = LSTM(100)(x1)

pos_input = Input(shape=(mxlen,), dtype="int32", name="pos_input")
x2 = Embedding(len(pos)+1, len(pos)+1, input_length=mxlen, weights=[pos_embeddings], trainable=False)(pos_input)
x2 = LSTM(100)(x2)

dep_input = Input(shape=(mxlen,), dtype="int32", name="dep_input")
x3 = Embedding(len(dep)+1, len(dep)+1, input_length=mxlen, weights=[dep_embeddings], trainable=False)(dep_input)
x3 = LSTM(100)(x3)

meta_input = Input(shape=(8699,), dtype="int32", name="meta_input")
x4 = Dense(64, activation="relu")(meta_input)

x = concatenate([x1, x2, x3, x4])

y = Dense(6, activation="softmax", name="main_output")(x)

model = Model(inputs=[word_input, pos_input, dep_input, meta_input], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"]) # have to look into it
model.fit({"word_input": x_train, "pos_input": x_train_pos, "dep_input": x_train_dep, "meta_input": x_train_meta}, {"main_output": y_train}, epochs=30, batch_size=40, verbose=1,
          validation_data=({"word_input": x_valid, "pos_input": x_valid_pos, "dep_input": x_valid_dep, "meta_input": x_valid_meta}, {"main_output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, x_test_pos, x_test_dep, x_test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

word_input = Input(shape=(mxlen,), dtype="int32", name="word_input")
x1 = Embedding(len(vocab), 100, input_length=mxlen, weights=[embeddings], trainable=False)(word_input)
x1 = Bidirectional(LSTM(100))(x1)

pos_input = Input(shape=(mxlen,), dtype="int32", name="pos_input")
x2 = Embedding(len(pos)+1, len(pos)+1, input_length=mxlen, weights=[pos_embeddings], trainable=False)(pos_input)
x2 = Bidirectional(LSTM(100))(x2)

dep_input = Input(shape=(mxlen,), dtype="int32", name="dep_input")
x3 = Embedding(len(dep)+1, len(dep)+1, input_length=mxlen, weights=[dep_embeddings], trainable=False)(dep_input)
x3 = Bidirectional(LSTM(100))(x3)

meta_input = Input(shape=(8699,), dtype="int32", name="meta_input")
x4 = Dense(64, activation="relu")(meta_input)

x = concatenate([x1, x2, x3, x4])

y = Dense(6, activation="softmax", name="main_output")(x)

model = Model(inputs=[word_input, pos_input, dep_input, meta_input], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"]) # have to look into it
model.fit({"word_input": x_train, "pos_input": x_train_pos, "dep_input": x_train_dep, "meta_input": x_train_meta}, {"main_output": y_train}, epochs=30, batch_size=40, verbose=1,
          validation_data=({"word_input": x_valid, "pos_input": x_valid_pos, "dep_input": x_valid_dep, "meta_input": x_valid_meta}, {"main_output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, x_test_pos, x_test_dep, x_test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

word_input = Input(shape=(mxlen,), dtype="int32", name="word_input")
x1 = Embedding(len(vocab), 100, input_length=mxlen, weights=[embeddings], trainable=False)(word_input)
x1 = GRU(100)(x1)

pos_input = Input(shape=(mxlen,), dtype="int32", name="pos_input")
x2 = Embedding(len(pos)+1, len(pos)+1, input_length=mxlen, weights=[pos_embeddings], trainable=False)(pos_input)
x2 = GRU(100)(x2)

dep_input = Input(shape=(mxlen,), dtype="int32", name="dep_input")
x3 = Embedding(len(dep)+1, len(dep)+1, input_length=mxlen, weights=[dep_embeddings], trainable=False)(dep_input)
x3 = GRU(100)(x3)

meta_input = Input(shape=(8699,), dtype="int32", name="meta_input")
x4 = Dense(64, activation="relu")(meta_input)

x = concatenate([x1, x2, x3, x4])

y = Dense(6, activation="softmax", name="main_output")(x)

model = Model(inputs=[word_input, pos_input, dep_input, meta_input], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"]) # have to look into it
model.fit({"word_input": x_train, "pos_input": x_train_pos, "dep_input": x_train_dep, "meta_input": x_train_meta}, {"main_output": y_train}, epochs=30, batch_size=40, verbose=1,
          validation_data=({"word_input": x_valid, "pos_input": x_valid_pos, "dep_input": x_valid_dep, "meta_input": x_valid_meta}, {"main_output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, x_test_pos, x_test_dep, x_test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

word_input = Input(shape=(mxlen,), dtype="int32", name="word_input")
x1 = Embedding(len(vocab), 100, input_length=mxlen, weights=[embeddings], trainable=False)(word_input)
x1 = Bidirectional(GRU(100))(x1)

pos_input = Input(shape=(mxlen,), dtype="int32", name="pos_input")
x2 = Embedding(len(pos)+1, len(pos)+1, input_length=mxlen, weights=[pos_embeddings], trainable=False)(pos_input)
x2 = Bidirectional(GRU(100))(x2)

dep_input = Input(shape=(mxlen,), dtype="int32", name="dep_input")
x3 = Embedding(len(dep)+1, len(dep)+1, input_length=mxlen, weights=[dep_embeddings], trainable=False)(dep_input)
x3 = Bidirectional(GRU(100))(x3)

meta_input = Input(shape=(8699,), dtype="int32", name="meta_input")
x4 = Dense(64, activation="relu")(meta_input)

x = concatenate([x1, x2, x3, x4])

y = Dense(6, activation="softmax", name="main_output")(x)

model = Model(inputs=[word_input, pos_input, dep_input, meta_input], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"]) # have to look into it
model.fit({"word_input": x_train, "pos_input": x_train_pos, "dep_input": x_train_dep, "meta_input": x_train_meta}, {"main_output": y_train}, epochs=30, batch_size=40, verbose=1,
          validation_data=({"word_input": x_valid, "pos_input": x_valid_pos, "dep_input": x_valid_dep, "meta_input": x_valid_meta}, {"main_output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, x_test_pos, x_test_dep, x_test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

word_input = Input(shape=(mxlen,), dtype="int32", name="word_input")
x1 = Embedding(len(vocab), 100, input_length=mxlen, weights=[embeddings], trainable=False)(word_input)
x1 = SimpleRNN(100)(x1)

pos_input = Input(shape=(mxlen,), dtype="int32", name="pos_input")
x2 = Embedding(len(pos)+1, len(pos)+1, input_length=mxlen, weights=[pos_embeddings], trainable=False)(pos_input)
x2 = SimpleRNN(100)(x2)

dep_input = Input(shape=(mxlen,), dtype="int32", name="dep_input")
x3 = Embedding(len(dep)+1, len(dep)+1, input_length=mxlen, weights=[dep_embeddings], trainable=False)(dep_input)
x3 = SimpleRNN(100)(x3)

meta_input = Input(shape=(8699,), dtype="int32", name="meta_input")
x4 = Dense(64, activation="relu")(meta_input)

x = concatenate([x1, x2, x3, x4])

y = Dense(6, activation="softmax", name="main_output")(x)

model = Model(inputs=[word_input, pos_input, dep_input, meta_input], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"]) # have to look into it
model.fit({"word_input": x_train, "pos_input": x_train_pos, "dep_input": x_train_dep, "meta_input": x_train_meta}, {"main_output": y_train}, epochs=30, batch_size=40, verbose=1,
          validation_data=({"word_input": x_valid, "pos_input": x_valid_pos, "dep_input": x_valid_dep, "meta_input": x_valid_meta}, {"main_output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, x_test_pos, x_test_dep, x_test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")

word_input = Input(shape=(mxlen,), dtype="int32", name="word_input")
x1 = Embedding(len(vocab), 100, input_length=mxlen, weights=[embeddings], trainable=False)(word_input)
x1 = Bidirectional(SimpleRNN(100))(x1)

pos_input = Input(shape=(mxlen,), dtype="int32", name="pos_input")
x2 = Embedding(len(pos)+1, len(pos)+1, input_length=mxlen, weights=[pos_embeddings], trainable=False)(pos_input)
x2 = Bidirectional(SimpleRNN(100))(x2)

dep_input = Input(shape=(mxlen,), dtype="int32", name="dep_input")
x3 = Embedding(len(dep)+1, len(dep)+1, input_length=mxlen, weights=[dep_embeddings], trainable=False)(dep_input)
x3 = Bidirectional(SimpleRNN(100))(x3)

meta_input = Input(shape=(8699,), dtype="int32", name="meta_input")
x4 = Dense(64, activation="relu")(meta_input)

x = concatenate([x1, x2, x3, x4])

y = Dense(6, activation="softmax", name="main_output")(x)

model = Model(inputs=[word_input, pos_input, dep_input, meta_input], outputs=[y])
adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(optimizer=adam,loss="categorical_crossentropy",metrics=["categorical_accuracy"]) # have to look into it
model.fit({"word_input": x_train, "pos_input": x_train_pos, "dep_input": x_train_dep, "meta_input": x_train_meta}, {"main_output": y_train}, epochs=30, batch_size=40, verbose=1,
          validation_data=({"word_input": x_valid, "pos_input": x_valid_pos, "dep_input": x_valid_dep, "meta_input": x_valid_meta}, {"main_output": y_valid}))

predictions = np.array([np.argmax(p) for p in model.predict([x_test, x_test_pos, x_test_dep, x_test_meta])])
correct = np.sum(predictions == y_test)
total = len(y_test)
print(f"Accuracy: {correct/total*100}")